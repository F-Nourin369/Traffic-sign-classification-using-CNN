{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F-Nourin369/Traffic-sign-classification-using-CNN/blob/main/inceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELhVcqrRQRp6",
        "outputId": "c6b534d9-30cd-44bd-b1d3-9fac1ceb961e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs6JdvCC9wxR",
        "outputId": "7746cc36-7bc3-489e-b9a9-6023139bb1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yqd_QQNSvl2"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpGRp1OVeyCB"
      },
      "source": [
        "set path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0oPUSQee5cc"
      },
      "outputs": [],
      "source": [
        "train_dir='/content/drive/MyDrive/newtrafficsign/train'\n",
        "validation_dir='/content/drive/MyDrive/newtrafficsign/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEygfqtgfTin"
      },
      "source": [
        "setting the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bNB7H19fXUY"
      },
      "outputs": [],
      "source": [
        "num_classes=85\n",
        "image_size=(299,299)\n",
        "batch_size=32\n",
        "learning_rate=0.001\n",
        "epochs=30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RqlYkoJghbg"
      },
      "source": [
        "preprocessing and augment the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw_i2fJogrgQ"
      },
      "outputs": [],
      "source": [
        "train_datagen=ImageDataGenerator(\n",
        "                 horizontal_flip=True,\n",
        "\n",
        "                 rescale=1. / 255,\n",
        "\n",
        "                 shear_range=0.2,\n",
        "                 zoom_range=0.2,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1PRO43dhnpG"
      },
      "source": [
        "preprocess the validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A3i3JEshudi"
      },
      "outputs": [],
      "source": [
        "valid_datagen=ImageDataGenerator(rescale=1 / 255,\n",
        "\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVcMP1npiCvt"
      },
      "source": [
        "load the InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwkIarkqiHyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7917f10f-e022-436a-f169-3cfb1e79e64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Initializing InceptionV3 (pretrained) model with input image shape\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(image_size[0],image_size[1],3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybNEgLgajuaI"
      },
      "source": [
        "adding the additional top layer for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ske59zAjPc3"
      },
      "outputs": [],
      "source": [
        "x=base_model.output\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "x=Dense(1024,activation='relu')(x)\n",
        "predictions=Dense(num_classes,activation='softmax')(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUmN0MWgj_VK"
      },
      "source": [
        "combine base model with  top layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6mwhMm5kEXV"
      },
      "outputs": [],
      "source": [
        "model=Model(inputs=base_model.input,outputs=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8R1PE7CkW5G"
      },
      "source": [
        "Freeze the layers in base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXCFfyYBkaIM"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "  layer.trainable=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzzyFgxfk_mZ"
      },
      "source": [
        "compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwhL0Z0slBlx"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(learning_rate=learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYy3tO2Gld0U"
      },
      "source": [
        "generate the training and validation data from directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9D-8wzKlkgC",
        "outputId": "d2979090-d773-495d-d85a-52d5ea7c8da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4438 images belonging to 85 classes.\n",
            "Found 1288 images belonging to 85 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_generator=train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_generator=valid_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeBQkkVmzgw"
      },
      "source": [
        "train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HckODiZMkefI",
        "outputId": "351a94b5-d0c8-4e5e-c45b-463ab17a48d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "138/138 [==============================] - 445s 3s/step - loss: 2.1962 - accuracy: 0.4444 - val_loss: 1.5528 - val_accuracy: 0.6195\n",
            "Epoch 2/30\n",
            "138/138 [==============================] - 180s 1s/step - loss: 1.1023 - accuracy: 0.6729 - val_loss: 1.1947 - val_accuracy: 0.6898\n",
            "Epoch 3/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.7849 - accuracy: 0.7671 - val_loss: 1.0258 - val_accuracy: 0.7133\n",
            "Epoch 4/30\n",
            "138/138 [==============================] - 174s 1s/step - loss: 0.6746 - accuracy: 0.7914 - val_loss: 0.9759 - val_accuracy: 0.7492\n",
            "Epoch 5/30\n",
            "138/138 [==============================] - 185s 1s/step - loss: 0.5579 - accuracy: 0.8173 - val_loss: 0.9786 - val_accuracy: 0.7227\n",
            "Epoch 6/30\n",
            "138/138 [==============================] - 179s 1s/step - loss: 0.4743 - accuracy: 0.8429 - val_loss: 1.0429 - val_accuracy: 0.7375\n",
            "Epoch 7/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.4429 - accuracy: 0.8493 - val_loss: 0.9091 - val_accuracy: 0.7625\n",
            "Epoch 8/30\n",
            "138/138 [==============================] - 187s 1s/step - loss: 0.3681 - accuracy: 0.8786 - val_loss: 1.0088 - val_accuracy: 0.7461\n",
            "Epoch 9/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.3377 - accuracy: 0.8872 - val_loss: 0.9748 - val_accuracy: 0.7586\n",
            "Epoch 10/30\n",
            "138/138 [==============================] - 174s 1s/step - loss: 0.3063 - accuracy: 0.9020 - val_loss: 0.8685 - val_accuracy: 0.7867\n",
            "Epoch 11/30\n",
            "138/138 [==============================] - 179s 1s/step - loss: 0.2721 - accuracy: 0.9047 - val_loss: 0.9331 - val_accuracy: 0.7766\n",
            "Epoch 12/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.2676 - accuracy: 0.9072 - val_loss: 0.8741 - val_accuracy: 0.7883\n",
            "Epoch 13/30\n",
            "138/138 [==============================] - 185s 1s/step - loss: 0.2447 - accuracy: 0.9151 - val_loss: 0.8648 - val_accuracy: 0.7812\n",
            "Epoch 14/30\n",
            "138/138 [==============================] - 172s 1s/step - loss: 0.2455 - accuracy: 0.9128 - val_loss: 0.8298 - val_accuracy: 0.8141\n",
            "Epoch 15/30\n",
            "138/138 [==============================] - 185s 1s/step - loss: 0.2242 - accuracy: 0.9194 - val_loss: 0.7889 - val_accuracy: 0.8102\n",
            "Epoch 16/30\n",
            "138/138 [==============================] - 183s 1s/step - loss: 0.2318 - accuracy: 0.9192 - val_loss: 0.8392 - val_accuracy: 0.7984\n",
            "Epoch 17/30\n",
            "138/138 [==============================] - 177s 1s/step - loss: 0.2074 - accuracy: 0.9301 - val_loss: 0.8326 - val_accuracy: 0.8109\n",
            "Epoch 18/30\n",
            "138/138 [==============================] - 178s 1s/step - loss: 0.1840 - accuracy: 0.9360 - val_loss: 0.8365 - val_accuracy: 0.8250\n",
            "Epoch 19/30\n",
            "138/138 [==============================] - 183s 1s/step - loss: 0.1788 - accuracy: 0.9374 - val_loss: 0.8326 - val_accuracy: 0.7937\n",
            "Epoch 20/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.2000 - accuracy: 0.9315 - val_loss: 0.8008 - val_accuracy: 0.8289\n",
            "Epoch 21/30\n",
            "138/138 [==============================] - 179s 1s/step - loss: 0.1628 - accuracy: 0.9451 - val_loss: 0.8358 - val_accuracy: 0.8242\n",
            "Epoch 22/30\n",
            "138/138 [==============================] - 183s 1s/step - loss: 0.1795 - accuracy: 0.9346 - val_loss: 0.8914 - val_accuracy: 0.8109\n",
            "Epoch 23/30\n",
            "138/138 [==============================] - 175s 1s/step - loss: 0.1598 - accuracy: 0.9453 - val_loss: 0.9011 - val_accuracy: 0.8094\n",
            "Epoch 24/30\n",
            "138/138 [==============================] - 185s 1s/step - loss: 0.1673 - accuracy: 0.9380 - val_loss: 0.7961 - val_accuracy: 0.8258\n",
            "Epoch 25/30\n",
            "138/138 [==============================] - 179s 1s/step - loss: 0.1526 - accuracy: 0.9510 - val_loss: 0.7746 - val_accuracy: 0.8344\n",
            "Epoch 26/30\n",
            "138/138 [==============================] - 186s 1s/step - loss: 0.1629 - accuracy: 0.9446 - val_loss: 0.8716 - val_accuracy: 0.8141\n",
            "Epoch 27/30\n",
            "138/138 [==============================] - 192s 1s/step - loss: 0.1491 - accuracy: 0.9492 - val_loss: 0.8461 - val_accuracy: 0.8148\n",
            "Epoch 28/30\n",
            "138/138 [==============================] - 184s 1s/step - loss: 0.1586 - accuracy: 0.9421 - val_loss: 0.8287 - val_accuracy: 0.8273\n",
            "Epoch 29/30\n",
            "138/138 [==============================] - 179s 1s/step - loss: 0.1406 - accuracy: 0.9480 - val_loss: 0.8011 - val_accuracy: 0.8383\n",
            "Epoch 30/30\n",
            "138/138 [==============================] - 188s 1s/step - loss: 0.1286 - accuracy: 0.9569 - val_loss: 0.8876 - val_accuracy: 0.8305\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.n // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=valid_generator.n// batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRwsTJvnz39"
      },
      "source": [
        "save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbxZJuqon3ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d262b52-a4b6-4e76-e061-7ef16b979377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/inceptionv3_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phUaqt4Lu2rS"
      },
      "source": [
        "Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c_hU_siBSbmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a419482f-4866-47a0-eb79-b9d794669b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.6.2)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gtts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rcnyDBeeIsd"
      },
      "outputs": [],
      "source": [
        "!pip install ipywebrtc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIW21NEG-3_H"
      },
      "outputs": [],
      "source": [
        "pip install ipywidgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFfEme2c_FRd"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import widgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIWcjgZodp5r",
        "outputId": "d02fc76f-e300-4cc9-d16d-b96f7eb331ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipywebrtc\n",
            "  Downloading ipywebrtc-0.6.0-py2.py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ipywebrtc\n",
            "Successfully installed ipywebrtc-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywebrtc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuhzFV6MSbdL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "from gtts import gTTS\n",
        "import ipywidgets as widgets\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/inceptionv3_model.h5')\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {0: 'ALL MOTOR VEHICLE PROHIBITED', 1: 'AXLE LOAD LIMIT', 2: 'BARRIER AHEAD', 3: 'BULLOCK AND HANDCART PROHIBITED', 4: 'Dragonfly', 5: 'CATTLE',\n",
        "                6: 'COMPULSARY AHEAD', 7: 'COMPULSARY AHEAD OR TURN LEFT', 8: 'COMPULSARY AHEAD OR TURN RIGHT', 9: 'COMPULSARY CYCLE TRACK',10: 'COMPULSARY KEEP RIGHT',11: 'COMPULSARY KEEP RIGHT',12: 'COMPULSARY MINIMUM SPEED',13: 'COMPULSARY SOUND HORN',14: 'COMPULSARY TURN LEFT',\n",
        "                15: 'COMPULSARY TURN LEFT AHEAD',16: 'COMPULSARY TURN RIGHT',17: 'COMPULSARY TURN RIGHT AHEAD',18: 'CROSS ROAD',19: 'CYCLE CROSSING',20: 'CYCLE PROHIBITED',21: 'DANGEROUS DIP',\n",
        "                22: 'DIRECTION',23: 'FALLING ROCKS',24: 'FERRY',25: 'GAP IN MEDIAN',26: 'GIVE WAY',27: 'GUARDED LEVEL CROSSING',28: 'HANDCART PROHIBITED',\n",
        "                29: 'HEIGHT LIMIT',30: 'HORN PROHIBITED',31: 'HUMP OR ROUGH ROAD',32: 'LEFT HAIR PIN BEND',33: 'LEFT HAND CURVE',34: 'LEFT REVERSE BEND',35: 'LEFT TURN PROHIBITED',\n",
        "                36: 'LENGTH LIMIT',37: 'LOAD LIMIT',38: 'LOOSE GRAVEL',39: 'MEN AT WORK',40: 'NARROW BRIDGE',41: 'NARROW ROAD AHEAD',42: 'NO ENTRY',\n",
        "                43: 'NO PARKING',44: 'NO STOPPING OR STANDING',45: 'OVERTAKING PROHIBITED',46: 'PASS EITHER SIDE',47: 'PEDESTRIAN CROSSING',48: 'PEDESTRIAN PROHIBITED',49: 'PRIORITY FOR ONCOMING VEHICLES',\n",
        "                50: 'QUAY SIDE OR RIVER BANK',51: 'RESTRICTION ENDS',52: 'RIGHT HAIR PIN BEND',53: 'RIGHT HAND CURVE',54: 'RIGHT REVERSE BEND',55: 'RIGHT TURN PROHIBITED',56: 'ROAD WIDENS AHEAD',\n",
        "                57: 'ROUNDABOUT',58: 'SCHOOL AHEAD',59: 'SIDE ROAD LEFT',60: 'SIDE ROAD RIGHT',61: 'SLIPPERY ROAD',62: 'SPEED LIMIT 15',63: 'SPEED LIMIT 20',\n",
        "                64: 'SPEED LIMIT 30',65: 'SPEED LIMIT 40',66: 'SPEED LIMIT 5',67: 'SPEED LIMIT 50',68: 'SPEED LIMIT 60',69: 'SPEED LIMIT 70',70: 'SPEED LIMIT 80',\n",
        "                71: 'STAGGERED INTERSECTION',72: 'STEEP ASCENT',73: 'STEEP DESCENT',74: 'STOP',75: 'STRAIGHT PROHIBITED',76: 'TONGA PROHIBITED',77: 'TRAFFIC SIGNAL',\n",
        "                78: 'TRUCK PROHIBITED',79: 'TURN RIGHT',80: 'T INTERSECTION',81: 'UNGUARDED LEVEL CROSSING',82: 'U TURN PROHIBITED',83: 'WIDTH LIMIT',84: 'Y INTERSECTION'}\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    img = PIL.Image.open(image_path)\n",
        "    img = img.resize((299, 299))\n",
        "    img = np.array(img) / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "# Function to predict the sign\n",
        "def predict_sign(image_path):\n",
        "    processed_image = preprocess_image(image_path)\n",
        "    predictions = model.predict(processed_image)\n",
        "    predicted_label = class_labels[np.argmax(predictions)]\n",
        "    confidence = np.max(predictions)\n",
        "    return predicted_label, confidence\n",
        "\n",
        "# Function to handle file upload option\n",
        "def upload_option(b):\n",
        "    uploaded = files.upload()\n",
        "    uploaded_file_path = list(uploaded.keys())[0]\n",
        "    prediction, confidence = predict_sign(uploaded_file_path)\n",
        "    img = PIL.Image.open(uploaded_file_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Prediction: {prediction}\\nConfidence: {confidence:.2%}')\n",
        "    plt.show()\n",
        "\n",
        "    # Generate voice alert\n",
        "    tts = gTTS(text=prediction, lang='en')\n",
        "    tts.save(\"prediction.mp3\")\n",
        "\n",
        "    # Display voice alert\n",
        "    ipd.display(ipd.Audio(\"prediction.mp3\", autoplay=True))\n",
        "\n",
        "# Create upload button\n",
        "upload_button = widgets.Button(description=\"Upload\")\n",
        "\n",
        "# Define button click event\n",
        "upload_button.on_click(upload_option)\n",
        "\n",
        "# Display button\n",
        "display(upload_button)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aixAGlX3SBFo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install ipywebrtc\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML\n",
        "from ipywebrtc import CameraStream\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Define the function to handle file uploads\n",
        "def handle_upload(change):\n",
        "    img = change['new'][0]\n",
        "    img_widget.value = img['content']\n",
        "\n",
        "# Create a camera stream object\n",
        "camera = CameraStream(constraints={'audio': False, 'video': True})\n",
        "\n",
        "# Create a file upload widget\n",
        "uploader = widgets.FileUpload(accept='.png,.jpg,.jpeg', multiple=False)\n",
        "\n",
        "# Create an output widget to display images or videos\n",
        "img_widget = display(None, display_id='image')\n",
        "\n",
        "# Function to display the uploaded image\n",
        "def display_image(change):\n",
        "    img = change['new']\n",
        "    img_widget.update(img)\n",
        "\n",
        "# Link the file uploader to the display image function\n",
        "uploader.observe(display_image, names='value')\n",
        "\n",
        "# Display the camera stream and file uploader\n",
        "display(HTML(\"<h1>Select an option:</h1>\"))\n",
        "display(HTML(\"<h2>Camera</h2>\"))\n",
        "display(camera)\n",
        "display(HTML(\"<h2>Upload</h2>\"))\n",
        "display(uploader)\n"
      ],
      "metadata": {
        "id": "IRTl8WUu_olq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJMv9VDUXrq0S3O5v2n8ZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}